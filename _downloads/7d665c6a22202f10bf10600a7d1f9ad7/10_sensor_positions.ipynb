{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Sensor positions\n\n.. important::\n\n    This example requires the ``meg_wiki`` package to download the sample dataset. This\n    package can be installed with ``pip``:\n\n```bash\n$ pip install git+https://github.com/fcbg-platforms/meg-wiki\n```\nThe sensor position is defined for each sensors as an `~numpy.array` of 12 elements.\nThis array represents the position and the normal given by a ``(3, 3)`` rotation matrix,\nin device coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mne import pick_info, set_log_level\nfrom mne._fiff.pick import _picks_to_idx  # handy private function for selection\nfrom mne.channels import read_layout\nfrom mne.io import read_info\nfrom mne.transforms import Transform\nfrom mne.viz import plot_alignment, plot_sensors, set_3d_view\nfrom mne.viz.backends.renderer import create_3d_figure\n\nfrom meg_wiki.datasets import sample\n\nset_log_level(\"WARNING\")\ninfo = read_info(\n    sample.data_path() / \"meas_info\" / \"measurement-info.fif\", verbose=False\n)\ninfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In MNE-Python, the sensor position is stored under the key ``loc`` for every channels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(info[\"chs\"][0][\"loc\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Along with other sensor information:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for key, value in info[\"chs\"][0].items():\n    if key == \"loc\":\n        print(f\"{key}:\\n  Position:\\n{value[:3]}\\n  Normal:\\n{value[3:].reshape(3, 3)}\")\n    else:\n        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sensors can be visualized on a 3D plot. This measurement information was taken\nfrom an empty-room recording. Thus, it does not contain an HPI measurement and it does\nnot contain the transform from the device to the head coordinate frame.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set an identity transformation from the device to head coordinates\ninfo[\"dev_head_t\"] = Transform(\"meg\", \"head\")\nax = plt.axes(projection=\"3d\")\nplot_sensors(info, kind=\"3d\", axes=ax)\nax.view_init(azim=130, elev=20)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 3D coordinates can be projected on a 2D plane to represent a topographic view of\nthe sensor array. In MNE, a :class:`~mne.channels.Layout` object is used to represent\nthe idealied 2D sensor positions. Built-in layouts are available for the MEGIN system,\nunder the keys ``'Vectorview-all'``, ``'Vectorview-mag'``, ``'Vectorview-grad'``, and\n``'Vectorview-grad_norm'``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layout = read_layout(\"Vectorview-all\")\nfig, ax = plt.subplots(1, 1, figsize=(16.53, 11.69), layout=\"constrained\")\nax.set(xticks=[], yticks=[], aspect=\"equal\")\noutlines = dict(border=([0, 1, 1, 0, 0], [0, 0, 1, 1, 0]))\nfor p, ch_name in zip(layout.pos, layout.names, strict=True):\n    center_pos = np.array((p[0] + p[2] / 2.0, p[1] + p[3] / 2.0))\n    ch_name = ch_name.split(\"MEG\")[1]\n    ch_name = f\"MAG\\n{ch_name}\" if ch_name.endswith(\"1\") else f\"GRAD\\n{ch_name}\"\n    ax.annotate(\n        ch_name,\n        xy=center_pos,\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n        size=6,\n    )\n    x1, x2, y1, y2 = p[0], p[0] + p[2], p[1], p[1] + p[3]\n    ax.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], color=\"k\")\nax.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. tip::\n\n   The function :func:`mne.viz.plot_layout` or the method\n   :meth:`mne.channels.Layout.plot` can be used to plot the layout with fewer lines\n   of code but without customization. For instance:\n\n```python\nfrom mne.channels import read_layout\n\nlayout = read_layout(\"Vectorview-all\")\nlayout.plot()\n```\nIf you pay attention to the plotted layout, you will notice that triplets of sensors\nare represented sometimes with ``GRADXXX2`` on top and ``GRADXXX3`` on the bottom, and\nsometimes the other way around. This is because the sensors are not all oriented in\nthe same direction. The sensor in the top box measures the derivative along the\nlatitude while the sensor in the bottom box measures the derivative along the\nlongitude.\n\n.. figure:: ../../../_static/meg/meg-channel-naming-convention.png\n    :align: center\n    :alt: MEG channel naming convention\n\n    Figure taken from MEGIN's User's Manual (copyright \u00a92011-2019 MEGIN Oy).\n\nWe can visualize the orientation in 3D, with the sensors oriented along the lines of\nlatitude (horizontal sensitivity along the equator) in **red** and the sensors\noriented along the lines of longitude (vertical sensitivity along the equator) in\n**orange**.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "picks = _picks_to_idx(info, picks=\"grad\")\ninfo = pick_info(info, picks, copy=False)\nmask_top = np.zeros(len(info.ch_names), bool)\nfor k, (ch2, ch3) in enumerate(\n    zip(info.ch_names[::2], info.ch_names[1::2], strict=True)\n):\n    # ch2 ends with MEGxxx2 and ch3 ends with MEGxxx3\n    idx2 = np.where(np.array(layout.names) == f\"MEG {ch2.split('MEG')[1]}\")[0]\n    idx3 = np.where(np.array(layout.names) == f\"MEG {ch3.split('MEG')[1]}\")[0]\n    mask_top[2 * k : 2 * k + 2] = (\n        [True, False]\n        if layout.pos[:, 1][idx2[0]] > layout.pos[:, 1][idx3[0]]\n        else [False, True]\n    )\nmask_bottom = ~mask_top  # opposite\n\n# retrieve position and orientation in device coordinate frame\npos = np.array([ch[\"loc\"][:3] for ch in info[\"chs\"]])\nori = np.array([ch[\"loc\"][3:6] for ch in info[\"chs\"]])\n\n# create render\nrenderer_kwargs = dict(bgcolor=\"w\")\nrenderer = create_3d_figure(\n    size=(800, 800),\n    scene=False,\n    bgcolor=\"w\",\n)\nplot_alignment(info, meg=\"sensors\", coord_frame=\"meg\", fig=renderer.scene())\nrenderer.quiver3d(*pos[mask_top].T, *ori[mask_top].T, \"r\", scale=0.015, mode=\"arrow\")\nrenderer.quiver3d(\n    *pos[mask_bottom].T, *ori[mask_bottom].T, \"orange\", scale=0.015, mode=\"arrow\"\n)\nset_3d_view(renderer.figure, azimuth=55, elevation=70, distance=0.55)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}